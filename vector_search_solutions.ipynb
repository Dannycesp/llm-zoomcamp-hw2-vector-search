{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a58be6",
   "metadata": {},
   "source": [
    "# Vector Search Homework Solutions\n",
    "\n",
    "This notebook contains solutions to the Vector Search homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1d4c4",
   "metadata": {},
   "source": [
    "## Q1. Embedding the query\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Embed the query: `'I just discovered the course. Can I join now?'`.\n",
    "Use the `'jinaai/jina-embeddings-v2-small-en'` model.\n",
    "\n",
    "You should get a numpy array of size 512.\n",
    "\n",
    "What's the minimal value in this array?\n",
    "\n",
    "- -0.51\n",
    "- -0.11\n",
    "- 0\n",
    "- 0.51\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec75e",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "We use FastEmbed's `TextEmbedding` with the specified model to embed the query.\n",
    "We check the shape of the resulting vector and find its minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e18b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (512,)\n",
      "Min value: -0.11726373885183883\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "def embed_text(text, model_name='jinaai/jina-embeddings-v2-small-en'):\n",
    "    model = TextEmbedding(model_name=model_name)\n",
    "    embedding = list(model.embed([text]))[0]\n",
    "    return np.array(embedding)\n",
    "\n",
    "query = 'I just discovered the course. Can I join now?'\n",
    "q = embed_text(query)\n",
    "print('Shape:', q.shape)\n",
    "print('Min value:', q.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dc771",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2. Cosine similarity with another vector\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Now let's embed this document:\n",
    "\n",
    "```python\n",
    "doc = 'Can I still join the course after the start date?'\n",
    "```\n",
    "\n",
    "What's the cosine similarity between the vector for the query and the vector for the document?\n",
    "\n",
    "- 0.3\n",
    "- 0.5\n",
    "- 0.7\n",
    "- 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775b560",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "We embed the document using the same model and compute the dot product with the query embedding.\n",
    "Since both vectors are normalized, the dot product gives the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b617b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9008528895674548\n"
     ]
    }
   ],
   "source": [
    "doc = 'Can I still join the course after the start date?'\n",
    "d = embed_text(doc)\n",
    "cosine_sim = float(np.dot(q, d))\n",
    "print('Cosine similarity:', cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ec6b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3. Ranking by cosine\n",
    "\n",
    "**Question:**\n",
    "\n",
    "For Q3 and Q4 we will use these documents:\n",
    "\n",
    "```python\n",
    "documents = [{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp'}, ...]\n",
    "```\n",
    "\n",
    "Compute the embeddings for the `text` field, and compute the cosine between the query vector and all the documents.\n",
    "\n",
    "What's the document index with the highest similarity? (Indexing starts from 0):\n",
    "\n",
    "- 0\n",
    "- 1\n",
    "- 2\n",
    "- 3\n",
    "- 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5ef23",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "We embed the `text` field of each document using the same model as before. We then compute the dot product between the query embedding (`q`) and the document embeddings matrix. The index of the maximum value in the resulting array corresponds to the most similar document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c18fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity index: 1\n"
     ]
    }
   ],
   "source": [
    "# Documents for Q3 and Q4\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'question': 'Course - Can I follow the course after it finishes?', 'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?', 'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'section': 'General course-related questions', 'question': 'Course - What can I do before the course starts?', 'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.', 'section': 'General course-related questions', 'question': 'How can we contribute to the course?', 'course': 'data-engineering-zoomcamp'}\n",
    "]\n",
    "\n",
    "# Embed the 'text' field of each document\n",
    "texts = [doc['text'] for doc in documents]\n",
    "V = np.array(list(TextEmbedding(model_name='jinaai/jina-embeddings-v2-small-en').embed(texts)))\n",
    "\n",
    "# Compute cosine similarities\n",
    "scores = V.dot(q)\n",
    "\n",
    "# Find the index of the highest score\n",
    "highest_score_index = np.argmax(scores)\n",
    "print(f\"Highest similarity index: {highest_score_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa6204",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q4. Ranking by cosine, version two\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Now let's calculate a new field, which is a concatenation of `question` and `text`:\n",
    "\n",
    "```python\n",
    "full_text = doc['question'] + ' ' + doc['text']\n",
    "```\n",
    "\n",
    "Embed this field and compute the cosine between it and the query vector. What's the highest scoring document?\n",
    "\n",
    "- 0\n",
    "- 1\n",
    "- 2\n",
    "- 3\n",
    "- 4\n",
    "\n",
    "Is it different from Q3? If yes, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d2460",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "We create a new field by concatenating the `question` and `text` fields for each document. This provides more context for the embedding model. We then embed these combined texts and compute their cosine similarity with the query vector. The document with the highest score is our answer. This approach can yield different and often more accurate results than using the `text` field alone because the question provides valuable keywords and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3da960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity index (full text): 0\n"
     ]
    }
   ],
   "source": [
    "# Create the 'full_text' field\n",
    "full_texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "# Embed the full texts\n",
    "V_full = np.array(list(TextEmbedding(model_name='jinaai/jina-embeddings-v2-small-en').embed(full_texts)))\n",
    "\n",
    "# Compute cosine similarities\n",
    "scores_full = V_full.dot(q)\n",
    "\n",
    "# Find the index of the highest score\n",
    "highest_score_index_full = np.argmax(scores_full)\n",
    "print(f\"Highest similarity index (full text): {highest_score_index_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7754c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q5. Selecting the embedding model\n",
    "\n",
    "**Question:**\n",
    "\n",
    "What's the smallest dimensionality for models in fastembed?\n",
    "\n",
    "- 128\n",
    "- 256\n",
    "- 384\n",
    "- 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddb2d1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The `fastembed` library provides a way to list all supported models and their metadata. We can iterate through this list to find the minimum dimensionality (`dim`) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c35d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embedding dimensions: [384, 512, 768, 1024]\n",
      "Smallest dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "models_list = TextEmbedding.list_supported_models()\n",
    "dims = sorted(set(m['dim'] for m in models_list))\n",
    "\n",
    "print('Available embedding dimensions:', dims)\n",
    "print('Smallest dimension:', min(dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f71b64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q6. Indexing with qdrant\n",
    "\n",
    "**Question:**\n",
    "\n",
    "For the last question, we will use more documents. We will select only FAQ records from our ml zoomcamp. Add them to qdrant using the model from Q5 (`BAAI/bge-small-en`).\n",
    "\n",
    "When adding the data, use both question and answer fields:\n",
    "```python\n",
    "text = doc['question'] + ' ' + doc['text']\n",
    "```\n",
    "\n",
    "After the data is inserted, use the question from Q1 for querying the collection. What's the highest score in the results?\n",
    "\n",
    "- 0.97\n",
    "- 0.87\n",
    "- 0.77\n",
    "- 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9dd24",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "First, we fetch a larger dataset and filter it for 'machine-learning-zoomcamp' documents. We initialize a new `TextEmbedding` model (`BAAI/bge-small-en`) and an in-memory `QdrantClient`. We then create a Qdrant collection with the correct vector size and cosine distance metric. We prepare the documents by combining their `question` and `text` fields, generate embeddings, and `upsert` them into the collection using `PointStruct`. Finally, we embed the original query with the new model and use the `search` method to find the most similar document and its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f4f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'machine-learning-zoomcamp' documents: 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202995/449802567.py:29: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest score in the search results is: 0.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202995/449802567.py:46: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Initialize the new embedding model\n",
    "new_embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# 1. Fetch Data\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "# 2. Filter Documents\n",
    "ml_zoomcamp_documents = [\n",
    "    {**doc, 'course': course['course']}\n",
    "    for course in documents_raw\n",
    "    if course['course'] == 'machine-learning-zoomcamp'\n",
    "    for doc in course['documents']\n",
    "]\n",
    "print(f\"Number of 'machine-learning-zoomcamp' documents: {len(ml_zoomcamp_documents)}\")\n",
    "\n",
    "# 3. Prepare documents for indexing\n",
    "texts_to_embed = [f\"{doc.get('question', '')} {doc.get('text', '')}\".strip() for doc in ml_zoomcamp_documents]\n",
    "\n",
    "# 4. Setup Qdrant Client and Collection\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"ml-zoomcamp-faq\"\n",
    "vector_size = len(list(new_embedding_model.embed(\"test\"))[0])\n",
    "\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n",
    ")\n",
    "\n",
    "# 5. Index the Data\n",
    "document_embeddings = list(new_embedding_model.embed(texts_to_embed))\n",
    "points = [\n",
    "    models.PointStruct(id=idx, vector=emb, payload=doc)\n",
    "    for idx, (doc, emb) in enumerate(zip(ml_zoomcamp_documents, document_embeddings))\n",
    "]\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points, wait=True)\n",
    "\n",
    "# 6. Search the collection\n",
    "query_q6 = 'I just discovered the course. Can I join now?'\n",
    "query_embedding_q6 = list(new_embedding_model.embed(query_q6))[0]\n",
    "\n",
    "search_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_embedding_q6,\n",
    "    limit=1,\n",
    ")\n",
    "\n",
    "# 7. Find Highest Score\n",
    "if search_results:\n",
    "    highest_score = search_results[0].score\n",
    "    print(f\"The highest score in the search results is: {highest_score:.4f}\")\n",
    "else:\n",
    "    print(\"No search results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea242e-c111-4096-ace5-31bbfd56890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using query_points instead of search as it is deprecated:\n",
    "#change query_vector to query in query_points (search_results function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265de-6521-4c23-bad7-8f5f889f28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# 1. Fetch Data\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "# 2. Filter Documents for 'machine-learning-zoomcamp'\n",
    "ml_zoomcamp_documents = []\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "    if course_name != 'machine-learning-zoomcamp':\n",
    "        continue\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        ml_zoomcamp_documents.append(doc)\n",
    "\n",
    "print(f\"Number of 'machine-learning-zoomcamp' documents: {len(ml_zoomcamp_documents)}\")\n",
    "\n",
    "# 3. Prepare Documents for Indexing\n",
    "texts_to_embed = []\n",
    "for doc in ml_zoomcamp_documents:\n",
    "    question_content = doc.get('question', '')\n",
    "    text_content = doc.get('text', '')\n",
    "    texts_to_embed.append(question_content + ' ' + text_content)\n",
    "\n",
    "# 4. Setup Qdrant Client\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Define the collection name\n",
    "collection_name = \"ml-zoomcamp-faq\"\n",
    "\n",
    "# Get the vector size from the embedding model\n",
    "dummy_embedding_q5 = list(new_embedding_model.embed(\"test\"))[0]\n",
    "vector_size_q5 = dummy_embedding_q5.shape[0]\n",
    "\n",
    "# Recreate collection\n",
    "if qdrant_client.collection_exists(collection_name=collection_name):\n",
    "    print(f\"Collection '{collection_name}' already exists. Deleting and recreating...\")\n",
    "    qdrant_client.delete_collection(collection_name=collection_name)\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(size=vector_size_q5, distance=models.Distance.COSINE),\n",
    ")\n",
    "print(f\"Collection '{collection_name}' created.\")\n",
    "\n",
    "# 5. Index the Data\n",
    "print(\"Generating embeddings for documents...\")\n",
    "document_embeddings_q5 = list(new_embedding_model.embed(texts_to_embed))\n",
    "print(\"Embeddings generated. Adding to Qdrant...\")\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    wait=True,\n",
    "    points=models.Batch(\n",
    "        ids=list(range(len(ml_zoomcamp_documents))),\n",
    "        vectors=document_embeddings_q5,\n",
    "        payloads=ml_zoomcamp_documents,\n",
    "    ),\n",
    ")\n",
    "print(f\"Added {len(ml_zoomcamp_documents)} documents to Qdrant collection '{collection_name}'.\")\n",
    "\n",
    "# 6. Search the Collection\n",
    "query_q6 = 'I just discovered the course. Can I join now?'\n",
    "query_embedding_q6 = list(new_embedding_model.embed(query_q6))[0]\n",
    "\n",
    "search_results = qdrant_client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_embedding_q6.tolist(),\n",
    "    limit=1,\n",
    "    with_payload=True,\n",
    ")\n",
    "\n",
    "# 7. Find Highest Score\n",
    "if search_results and search_results.points:\n",
    "    highest_score = search_results.points[0].score\n",
    "    print(f\"The highest score in the search results is: {highest_score:.4f}\")\n",
    "else:\n",
    "    print(\"No search results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa109d5-3ec0-4f29-846d-a3f1255d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modular version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a722c-b452-4ffe-9381-d277a52e7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "def fetch_documents(url: str) -> list[dict]:\n",
    "    \"\"\"Fetches and parses JSON documents from a URL.\"\"\"\n",
    "    print(f\"Fetching documents from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    print(\"Documents fetched successfully.\")\n",
    "    return response.json()\n",
    "\n",
    "def filter_documents_by_course(documents_raw: list[dict], course_name: str) -> list[dict]:\n",
    "    \"\"\"Filters documents for a specific course.\"\"\"\n",
    "    return [\n",
    "        {**doc, 'course': course['course']}\n",
    "        for course in documents_raw\n",
    "        if course['course'] == course_name\n",
    "        for doc in course['documents']\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    # 1. Configuration\n",
    "    docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "    course_to_filter = 'machine-learning-zoomcamp'\n",
    "    collection_name = \"ml_zoomcamp_faq_simplified\"\n",
    "    embedding_model_name = \"BAAI/bge-small-en\"\n",
    "    search_query = 'I just discovered the course. Can I join now?'\n",
    "\n",
    "    # 2. Initialize models and clients\n",
    "    embedding_model = TextEmbedding(model_name=embedding_model_name)\n",
    "    qdrant_client = QdrantClient(\":memory:\")  # In-memory storage\n",
    "\n",
    "    # 3. Fetch and process data\n",
    "    documents_raw = fetch_documents(docs_url)\n",
    "    ml_documents = filter_documents_by_course(documents_raw, course_to_filter)\n",
    "    print(f\"Found {len(ml_documents)} documents for '{course_to_filter}'.\")\n",
    "\n",
    "    texts_to_embed = [\n",
    "        f\"{doc.get('question', '')} {doc.get('text', '')}\".strip()\n",
    "        for doc in ml_documents\n",
    "    ]\n",
    "\n",
    "    # 4. Setup and create Qdrant collection\n",
    "    print(\"Setting up Qdrant collection...\")\n",
    "    # Use the model to get the vector size\n",
    "    vector_size = len(list(embedding_model.embed(\"A test sentence to get vector size\"))[0])\n",
    "\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created with vector size {vector_size}.\")\n",
    "\n",
    "    # 5. Generate embeddings and index the documents\n",
    "    print(\"Generating embeddings and indexing documents...\")\n",
    "    embeddings = list(embedding_model.embed(texts_to_embed))\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            models.PointStruct(id=idx, vector=emb, payload=doc)\n",
    "            for idx, (doc, emb) in enumerate(zip(ml_documents, embeddings))\n",
    "        ],\n",
    "        wait=True,\n",
    "    )\n",
    "    print(f\"Indexed {len(ml_documents)} documents.\")\n",
    "\n",
    "    # 6. Perform the search\n",
    "    print(f\"Searching for: '{search_query}'\")\n",
    "    query_embedding = list(embedding_model.embed(search_query))[0]\n",
    "\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=1,  # Get the top result\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # 7. Display the top result\n",
    "    if search_results:\n",
    "        top_result = search_results[0]\n",
    "        print(f\"\\nHighest score: {top_result.score:.4f}\")\n",
    "        print(\"Top result payload:\")\n",
    "        print(top_result.payload)\n",
    "    else:\n",
    "        print(\"No search results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7abd56-5fd0-46d6-a7e3-7afce5e8510b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.10.12(.env_dataeng)",
   "language": "python",
   "name": ".env_dataeng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
